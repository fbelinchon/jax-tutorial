{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "permanent-medicaid",
   "metadata": {
    "papermill": {
     "duration": 0.025811,
     "end_time": "2022-10-05T14:06:59.897170",
     "exception": false,
     "start_time": "2022-10-05T14:06:59.871359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Update - 23rd Dec, 2021**\n",
    "\n",
    "We have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n",
    "\n",
    "### TensorFlow Notebooks:\n",
    "\n",
    "* [TF_JAX_Tutorials - Part 1](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part1)\n",
    "* [TF_JAX_Tutorials - Part 2](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part2)\n",
    "* [TF_JAX_Tutorials - Part 3](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part3)\n",
    "\n",
    "### JAX Notebooks:\n",
    "\n",
    "* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-4-jax-and-devicearray)\n",
    "* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-5-pure-functions-in-jax/)\n",
    "* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-6-prng-in-jax/)\n",
    "* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-7-jit-in-jax)\n",
    "* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-8-vmap-pmap)\n",
    "* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-9-autodiff-in-jax)\n",
    "* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-10-pytrees-in-jax)\n",
    "\n",
    "### Github Repo with all notebooks in one place\n",
    "https://github.com/AakashKumarNain/TF_JAX_tutorials\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/yjprpOoH5c8/maxresdefault.jpg\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "Welcome to another TensorFlow/JAX tutorial. This is the third tutorial in this series. If you haven't looked at the previous tutorials,\n",
    "I would highly recommend checking them out.\n",
    "\n",
    "1. [TF-JAX Tutorials - Part 1](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part1)\n",
    "2. [TF-JAX Tutorials - Part 2](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part2)\n",
    "\n",
    "**Note** The tutorials are in the following format:\n",
    "1. TF Fundamentals (2-3 notebooks)\n",
    "2. JAX Fundamentals (2-3 notebooks)\n",
    "3. Advanced TF (2-3 notebooks)\n",
    "4. Advanced JAX (2-3 notebooks)\n",
    "\n",
    "\n",
    "Today we will be taking a deep dive into a very important topic: **`Gradients`**\n",
    "\n",
    "`Automatic Differentiation` and `Gradients` are so important concepts that they deserve a few dedicated chapters. Understanding every bit of it isn't necessary but the more you dive into it, the more you will appreciate the beauty of it. I am planning to do an `advanced` tutorial on these topics if there is enough interest from the readers. Do let me know in the comments section what you think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adopted-painting",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-05T14:06:59.951987Z",
     "iopub.status.busy": "2022-10-05T14:06:59.950629Z",
     "iopub.status.idle": "2022-10-05T14:07:06.705620Z",
     "shell.execute_reply": "2022-10-05T14:07:06.704735Z",
     "shell.execute_reply.started": "2022-10-05T13:39:20.330732Z"
    },
    "papermill": {
     "duration": 6.783501,
     "end_time": "2022-10-05T14:07:06.705798",
     "exception": false,
     "start_time": "2022-10-05T14:06:59.922297",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "seed=1234\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-range",
   "metadata": {
    "papermill": {
     "duration": 0.024288,
     "end_time": "2022-10-05T14:07:06.754588",
     "exception": false,
     "start_time": "2022-10-05T14:07:06.730300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Automatic Differentiation and Gradients\n",
    "\n",
    "Let's say you apply a sequence of operations on an input in a *forward* pass. To differentiate automatically, you need some sort of mechanism to \n",
    "figure out:\n",
    "1. What operations were applied in the forward pass?\n",
    "2. What was the order in which the operations were applied?\n",
    "\n",
    "For autodiff, you need to remember the above two. Different frameworks can implement the same idea in different ways but the fundamentals remain the same.\n",
    "\n",
    "\n",
    "### Gradients in TensorFlow\n",
    "\n",
    "TensorFlow provides the `tf.GradientTape` API for automatic differentiation. Any relevant operation executed inside the context of `GradientTape` gets recorded for gradients computation. To compute gradients, you need to do the following:\n",
    "\n",
    "1. Record operations inside the `tf.GradientTape` context\n",
    "2. Compute the gradients using `GradientTape.gradient(target, sources)`\n",
    "\n",
    "Let's code up a few examples for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "remarkable-narrow",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:06.809605Z",
     "iopub.status.busy": "2022-10-05T14:07:06.808745Z",
     "iopub.status.idle": "2022-10-05T14:07:06.844465Z",
     "shell.execute_reply": "2022-10-05T14:07:06.845065Z",
     "shell.execute_reply.started": "2022-10-05T13:41:57.817477Z"
    },
    "papermill": {
     "duration": 0.066532,
     "end_time": "2022-10-05T14:07:06.845255",
     "exception": false,
     "start_time": "2022-10-05T14:07:06.778723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable x: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>\n",
      "Is x trainable?: True\n",
      "\n",
      "Variable y: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0>\n",
      "Is y trainable?: True\n"
     ]
    }
   ],
   "source": [
    "# We will initialize a few Variables here\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(4.0)\n",
    "\n",
    "print(f\"Variable x: {x}\")\n",
    "print(f\"Is x trainable?: {x.trainable}\")\n",
    "print(f\"\\nVariable y: {y}\")\n",
    "print(f\"Is y trainable?: {y.trainable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-landing",
   "metadata": {
    "papermill": {
     "duration": 0.02385,
     "end_time": "2022-10-05T14:07:06.893677",
     "exception": false,
     "start_time": "2022-10-05T14:07:06.869827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will do a simple operation here: **`z = x * y`**. And we will calculate the gradients of `z` wrt `x` and `y` . We are taking a simple example so that the readers can easily verify that things are working as expected. We will work on complex examples in a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foreign-channels",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:06.945136Z",
     "iopub.status.busy": "2022-10-05T14:07:06.944439Z",
     "iopub.status.idle": "2022-10-05T14:07:06.964818Z",
     "shell.execute_reply": "2022-10-05T14:07:06.965389Z",
     "shell.execute_reply.started": "2022-10-05T13:42:28.544635Z"
    },
    "papermill": {
     "duration": 0.047728,
     "end_time": "2022-10-05T14:07:06.965571",
     "exception": false,
     "start_time": "2022-10-05T14:07:06.917843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Variable x: 3.0\n",
      "Input Variable y: 4.0\n",
      "Output z: 12.0\n",
      "\n",
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Remember we need to execute the operations inside the context\n",
    "# of GradientTape so that we can record them\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x * y\n",
    "    \n",
    "dx, dy = tape.gradient(z, [x, y])\n",
    "\n",
    "print(f\"Input Variable x: {x.numpy()}\")\n",
    "print(f\"Input Variable y: {y.numpy()}\")\n",
    "print(f\"Output z: {z}\\n\")\n",
    "\n",
    "# dz / dx\n",
    "print(f\"Gradient of z wrt x: {dx}\")\n",
    "\n",
    "# dz / dy\n",
    "print(f\"Gradient of z wrt y: {dy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "democratic-cameroon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.019273Z",
     "iopub.status.busy": "2022-10-05T14:07:07.018576Z",
     "iopub.status.idle": "2022-10-05T14:07:07.036990Z",
     "shell.execute_reply": "2022-10-05T14:07:07.037548Z",
     "shell.execute_reply.started": "2022-10-05T13:46:30.398917Z"
    },
    "papermill": {
     "duration": 0.047579,
     "end_time": "2022-10-05T14:07:07.037748",
     "exception": false,
     "start_time": "2022-10-05T14:07:06.990169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Variable x: 3.0\n",
      "Input Variable y: 4.0\n",
      "Output z: 338.0\n",
      "\n",
      "Gradient of z wrt x: 12.0\n",
      "Gradient of z wrt y: 240.0\n"
     ]
    }
   ],
   "source": [
    "# Remember we need to execute the operations inside the context\n",
    "# of GradientTape so that we can record them\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = 2*tf.pow(x,2) +5*tf.pow(y,3)\n",
    "    \n",
    "dx, dy = tape.gradient(z, [x, y])\n",
    "\n",
    "print(f\"Input Variable x: {x.numpy()}\")\n",
    "print(f\"Input Variable y: {y.numpy()}\")\n",
    "print(f\"Output z: {z}\\n\")\n",
    "\n",
    "# dz / dx = 4*x=12\n",
    "print(f\"Gradient of z wrt x: {dx}\")\n",
    "\n",
    "# dz / dy =15*y^2=240\n",
    "print(f\"Gradient of z wrt y: {dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-orlando",
   "metadata": {
    "papermill": {
     "duration": 0.025586,
     "end_time": "2022-10-05T14:07:07.088529",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.062943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Easy enough! Similarly, you can calculate gradients of many many variables wrt to some computation say `loss` by just passing all the trainable variables involved in that computation in a nested way (can be a list or dictionary for example). The returned gradients will follow the same nested structure in which the inputs are passed to the tape.\n",
    "\n",
    "What happens if we calculate the gradients in the above code wrt x and y separately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surrounded-henry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.148593Z",
     "iopub.status.busy": "2022-10-05T14:07:07.147781Z",
     "iopub.status.idle": "2022-10-05T14:07:07.152851Z",
     "shell.execute_reply": "2022-10-05T14:07:07.153453Z",
     "shell.execute_reply.started": "2022-10-05T13:43:31.095855Z"
    },
    "papermill": {
     "duration": 0.037738,
     "end_time": "2022-10-05T14:07:07.153637",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.115899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! ERROR! ERROR!\n",
      "\n",
      "RuntimeError A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = x * y\n",
    "\n",
    "try:\n",
    "    dx = tape.gradient(z, x)\n",
    "    dy = tape.gradient(z, y)\n",
    "\n",
    "    print(f\"Gradient of z wrt x: {dx}\")\n",
    "    print(f\"Gradient of z wrt y: {dy}\")\n",
    "except Exception as ex:\n",
    "    print(\"ERROR! ERROR! ERROR!\\n\")\n",
    "    print(type(ex).__name__, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-union",
   "metadata": {
    "papermill": {
     "duration": 0.024996,
     "end_time": "2022-10-05T14:07:07.204481",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.179485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What just happened?**<br>\n",
    "As soon as the `GradientTape.gradient(...)` is called, all the resources held by a `GradientTape` are released. So, if you computed the `gradient` once, then you won't be able to call it again.\n",
    "\n",
    "**What's the solution then?**<br>\n",
    "The solution is to use set the `persistent` argument to `True`. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. Let's try the above example again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interstate-tyler",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.261794Z",
     "iopub.status.busy": "2022-10-05T14:07:07.261076Z",
     "iopub.status.idle": "2022-10-05T14:07:07.266062Z",
     "shell.execute_reply": "2022-10-05T14:07:07.265543Z",
     "shell.execute_reply.started": "2022-10-05T13:44:20.482772Z"
    },
    "papermill": {
     "duration": 0.036653,
     "end_time": "2022-10-05T14:07:07.266212",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.229559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Set the persistent argument\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = x * y\n",
    "\n",
    "try:\n",
    "    dx = tape.gradient(z, x)\n",
    "    dy = tape.gradient(z, y)\n",
    "\n",
    "    print(f\"Gradient of z wrt x: {dx}\")\n",
    "    print(f\"Gradient of z wrt y: {dy}\")\n",
    "except Exception as ex:\n",
    "    print(\"ERROR! ERROR! ERROR!\\n\")\n",
    "    print(type(ex).__name__, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conventional-meaning",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.321433Z",
     "iopub.status.busy": "2022-10-05T14:07:07.320690Z",
     "iopub.status.idle": "2022-10-05T14:07:07.331316Z",
     "shell.execute_reply": "2022-10-05T14:07:07.331901Z",
     "shell.execute_reply.started": "2022-10-05T13:48:28.076869Z"
    },
    "papermill": {
     "duration": 0.040276,
     "end_time": "2022-10-05T14:07:07.332111",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.291835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable x: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>\n",
      "Is x trainable?: True\n",
      "\n",
      "Variable y: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0>\n",
      "Is y trainable?: False\n",
      "\n",
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: None\n"
     ]
    }
   ],
   "source": [
    "# What if one of the Variables is non-trainable?\n",
    "# Let's make y non-trainable in the above example and run\n",
    "# the computation again\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(4.0, trainable=False)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x * y\n",
    "    \n",
    "dx, dy = tape.gradient(z, [x, y])\n",
    "\n",
    "print(f\"Variable x: {x}\")\n",
    "print(f\"Is x trainable?: {x.trainable}\")\n",
    "print(f\"\\nVariable y: {y}\")\n",
    "print(f\"Is y trainable?: {y.trainable}\\n\")\n",
    "\n",
    "print(f\"Gradient of z wrt x: {dx}\")\n",
    "print(f\"Gradient of z wrt y: {dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-mounting",
   "metadata": {
    "papermill": {
     "duration": 0.025865,
     "end_time": "2022-10-05T14:07:07.384584",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.358719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Note:** An important point to remember is that you should never mix the `topology` of the `dtypes` for AD and computing the graidents. When I say `topology`, I mean don't mix `float`, `int`, `string` types. In fact, you can't take a gradient for any op that has a dtype of `int` or `string`. Let us take an example to make this clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "allied-bidding",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.444906Z",
     "iopub.status.busy": "2022-10-05T14:07:07.444186Z",
     "iopub.status.idle": "2022-10-05T14:07:07.453846Z",
     "shell.execute_reply": "2022-10-05T14:07:07.454390Z",
     "shell.execute_reply.started": "2022-10-05T13:49:01.018067Z"
    },
    "papermill": {
     "duration": 0.044341,
     "end_time": "2022-10-05T14:07:07.454572",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.410231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Variable x: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>\n",
      "Input Variable y: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=4>\n",
      "Output z: 12.0\n",
      "\n",
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: None\n"
     ]
    }
   ],
   "source": [
    "# Note the dtypes\n",
    "\n",
    "x = tf.Variable(3.0, dtype=tf.float32)\n",
    "y = tf.Variable(4, dtype=tf.int32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x * tf.cast(y, x.dtype)\n",
    "    \n",
    "dx, dy = tape.gradient(z, [x, y])\n",
    "\n",
    "print(f\"Input Variable x: {x}\")\n",
    "print(f\"Input Variable y: {y}\")\n",
    "print(f\"Output z: {z}\\n\")\n",
    "\n",
    "print(f\"Gradient of z wrt x: {dx}\")\n",
    "print(f\"Gradient of z wrt y: {dy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "informative-funds",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.515399Z",
     "iopub.status.busy": "2022-10-05T14:07:07.514516Z",
     "iopub.status.idle": "2022-10-05T14:07:07.521491Z",
     "shell.execute_reply": "2022-10-05T14:07:07.522379Z",
     "shell.execute_reply.started": "2022-10-05T13:49:43.817635Z"
    },
    "papermill": {
     "duration": 0.041924,
     "end_time": "2022-10-05T14:07:07.522624",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.480700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Variable x: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n",
      "Input Variable y: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=4>\n",
      "Output z: 12\n",
      "\n",
      "Gradient of z wrt x: None\n",
      "Gradient of z wrt y: None\n"
     ]
    }
   ],
   "source": [
    "# There is no gradient flow defined for int and string types\n",
    "\n",
    "x = tf.Variable(3, dtype=tf.int32)\n",
    "y = tf.Variable(4, dtype=tf.int32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x * y\n",
    "    \n",
    "dx, dy = tape.gradient(z, [x, y])\n",
    "\n",
    "print(f\"Input Variable x: {x}\")\n",
    "print(f\"Input Variable y: {y}\")\n",
    "print(f\"Output z: {z}\\n\")\n",
    "\n",
    "print(f\"Gradient of z wrt x: {dx}\")\n",
    "print(f\"Gradient of z wrt y: {dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-saint",
   "metadata": {
    "papermill": {
     "duration": 0.026175,
     "end_time": "2022-10-05T14:07:07.575923",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.549748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You would say that your code is correct, which in some sense is, but you will get `None` as the gradient value for the variables where the data types of the source and the target are different. Before we move to a very important concept, let's summarize all the things we have learned so far:\n",
    "\n",
    "1. `tf.GradientTape` is the API for doing AD in TensorFlow\n",
    "2. For computing gradients using Tape, we need:\n",
    "   * Record the relevant operations inside the context of Tape\n",
    "   * Compute the gradients by calling the `GradientTape.gradient(...)` method\n",
    "3. If you wish to call the `gradient(...)` method multiple times, make sure to set the `persistent` argument to `GradientTape`\n",
    "4. If any `non-trainable` variable is involved in the computation, then the gradient wrt to that variable would be `None`\n",
    "5. Mixing `dtype` topology is a blunder. Your code will run but will fail silently!\n",
    "\n",
    "\n",
    "### Fine-gain control\n",
    "\n",
    "A few questions that comes to mind naturally after seeing the above examples:\n",
    "\n",
    "1. How to access all the objects that are being watched?\n",
    "2. How to stop the flow of a gradient through a specific Variable/path?\n",
    "3. What if you don't want to watch all the variables inside the `GradientTape` context?\n",
    "4. What if you want to watch something that isn't inside the context?\n",
    "\n",
    "We will take a few examples for each of the above case to understand it in a better way.\n",
    "\n",
    "#### 1. Accessing all the watched objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "northern-shower",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.632831Z",
     "iopub.status.busy": "2022-10-05T14:07:07.631784Z",
     "iopub.status.idle": "2022-10-05T14:07:07.648222Z",
     "shell.execute_reply": "2022-10-05T14:07:07.648788Z",
     "shell.execute_reply.started": "2022-10-05T13:51:38.189607Z"
    },
    "papermill": {
     "duration": 0.04651,
     "end_time": "2022-10-05T14:07:07.648981",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.602471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 't:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[ 0.8369314 , -0.7342977 ],\n",
      "       [ 1.0402943 ,  0.04035992]], dtype=float32)>\n",
      "Tape is watching all of these:\n",
      "x:0 and it's value is 3.0\n",
      "y:0 and it's value is 4.0\n",
      "t:0 and it's value is [[ 0.8369314  -0.7342977 ]\n",
      " [ 1.0402943   0.04035992]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0, name=\"x\")\n",
    "y = tf.Variable(4.0, name=\"y\")\n",
    "t = tf.Variable(tf.random.normal(shape=(2, 2)), name=\"t\")\n",
    "print(t)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x * y +t\n",
    "\n",
    "print(\"Tape is watching all of these:\")\n",
    "for var in tape.watched_variables():\n",
    "    print(f\"{var.name} and it's value is {var.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-cross",
   "metadata": {
    "papermill": {
     "duration": 0.027279,
     "end_time": "2022-10-05T14:07:07.703353",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.676074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Stopping the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "southeast-festival",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.764694Z",
     "iopub.status.busy": "2022-10-05T14:07:07.763730Z",
     "iopub.status.idle": "2022-10-05T14:07:07.775574Z",
     "shell.execute_reply": "2022-10-05T14:07:07.774836Z",
     "shell.execute_reply.started": "2022-10-05T13:52:34.186403Z"
    },
    "papermill": {
     "duration": 0.045117,
     "end_time": "2022-10-05T14:07:07.775725",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.730608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: 3.0\n",
      "\n",
      "Gradient of zz wrt x: None\n",
      "Gradient of zz wrt y: None\n"
     ]
    }
   ],
   "source": [
    "# The ugly way\n",
    "\n",
    "x = tf.Variable(3.0, name=\"x\")\n",
    "y = tf.Variable(4.0, name=\"y\")\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = x * y\n",
    "    \n",
    "    # Stop the grasdient flow\n",
    "    with tape.stop_recording():\n",
    "        zz = x*x + y*y\n",
    "\n",
    "dz_dx, dz_dy = tape.gradient(z, [x, y])\n",
    "dzz_dx, dzz_dy = tape.gradient(zz, [x, y])\n",
    "\n",
    "print(f\"Gradient of z wrt x: {dz_dx}\")\n",
    "print(f\"Gradient of z wrt y: {dz_dy}\\n\")\n",
    "print(f\"Gradient of zz wrt x: {dzz_dx}\")\n",
    "print(f\"Gradient of zz wrt y: {dzz_dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-marketplace",
   "metadata": {
    "papermill": {
     "duration": 0.026658,
     "end_time": "2022-10-05T14:07:07.829999",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.803341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A better way to stop gradient flow is to use `tf.stop_gradient(...)`. Why?\n",
    "1. Doesn't require access to tape\n",
    "2. Clean with much better semantics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "finite-drill",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:07.888254Z",
     "iopub.status.busy": "2022-10-05T14:07:07.887251Z",
     "iopub.status.idle": "2022-10-05T14:07:07.896340Z",
     "shell.execute_reply": "2022-10-05T14:07:07.896889Z",
     "shell.execute_reply.started": "2022-10-05T13:53:04.171961Z"
    },
    "papermill": {
     "duration": 0.039678,
     "end_time": "2022-10-05T14:07:07.897095",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.857417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: None\n"
     ]
    }
   ],
   "source": [
    "# The better way!\n",
    "\n",
    "x = tf.Variable(3.0, name=\"x\")\n",
    "y = tf.Variable(4.0, name=\"y\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x * tf.stop_gradient(y)\n",
    "\n",
    "dz_dx, dz_dy = tape.gradient(z, [x, y])\n",
    "print(f\"Gradient of z wrt x: {dz_dx}\")\n",
    "print(f\"Gradient of z wrt y: {dz_dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-montana",
   "metadata": {
    "papermill": {
     "duration": 0.027465,
     "end_time": "2022-10-05T14:07:07.952113",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.924648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. Select what to watch inside the context?\n",
    "\n",
    "By default `GradientTape` will automatically watch any trainable variables that are accessed inside the context but if you want gradients for selected variables only, then you can disable automatic tracking by passing `watch_accessed_variables=False` to the tape constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecological-practice",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.010387Z",
     "iopub.status.busy": "2022-10-05T14:07:08.009423Z",
     "iopub.status.idle": "2022-10-05T14:07:08.019589Z",
     "shell.execute_reply": "2022-10-05T14:07:08.020271Z",
     "shell.execute_reply.started": "2022-10-05T13:56:51.212021Z"
    },
    "papermill": {
     "duration": 0.041031,
     "end_time": "2022-10-05T14:07:08.020518",
     "exception": false,
     "start_time": "2022-10-05T14:07:07.979487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z wrt x: None\n",
      "Gradient of z wrt y: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Both variables are trainable\n",
    "x = tf.Variable(3.0, name=\"x\")\n",
    "y = tf.Variable(4.0, name=\"y\")\n",
    "\n",
    "# Telling the tape: Hey! I will tell you what to record.\n",
    "# Don't start recording automatically!\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    # Watch x but not y\n",
    "    tape.watch(y)\n",
    "    z = x * y\n",
    "dz_dx, dz_dy = tape.gradient(z, [x, y])\n",
    "\n",
    "\n",
    "print(f\"Gradient of z wrt x: {dz_dx}\")\n",
    "print(f\"Gradient of z wrt y: {dz_dy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ideal-surge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.084781Z",
     "iopub.status.busy": "2022-10-05T14:07:08.084049Z",
     "iopub.status.idle": "2022-10-05T14:07:08.090373Z",
     "shell.execute_reply": "2022-10-05T14:07:08.090969Z",
     "shell.execute_reply.started": "2022-10-05T13:54:32.814933Z"
    },
    "papermill": {
     "duration": 0.041173,
     "end_time": "2022-10-05T14:07:08.091161",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.049988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tape watching only these objects that you asked it to watch\n",
      "x:0 and it's value is 3.0\n",
      "t:0 and it's value is 5.0\n"
     ]
    }
   ],
   "source": [
    "# What if something that you wanted to watch,\n",
    "# wasn't present in the computation done inside \n",
    "# the context?\n",
    "\n",
    "x = tf.Variable(3.0, name=\"x\")\n",
    "y = tf.Variable(4.0, name=\"y\")\n",
    "t = tf.Variable(5.0, name=\"t\")\n",
    "\n",
    "# Telling the tape: Hey! I will tell you what to record.\n",
    "# Don't start recording automatically!\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    # Watch x but not y\n",
    "    tape.watch(x)\n",
    "    z = x * y\n",
    "    \n",
    "    # `t` isn't involved in any computation here\n",
    "    # but what if we want to record it as well\n",
    "    tape.watch(t)\n",
    "\n",
    "print(\"Tape watching only these objects that you asked it to watch\")\n",
    "for var in tape.watched_variables():\n",
    "    print(f\"{var.name} and it's value is {var.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-anderson",
   "metadata": {
    "papermill": {
     "duration": 0.027885,
     "end_time": "2022-10-05T14:07:08.147046",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.119161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Multiple Tapes\n",
    "You can use more than one `GradientTape` for recording different objects. Tapes interact seamlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "incorrect-flavor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.206577Z",
     "iopub.status.busy": "2022-10-05T14:07:08.205909Z",
     "iopub.status.idle": "2022-10-05T14:07:08.216315Z",
     "shell.execute_reply": "2022-10-05T14:07:08.215605Z",
     "shell.execute_reply.started": "2022-10-05T13:55:07.646486Z"
    },
    "papermill": {
     "duration": 0.041093,
     "end_time": "2022-10-05T14:07:08.216469",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.175376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: 3.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0, name=\"x\")\n",
    "y = tf.Variable(4.0, name=\"y\")\n",
    "\n",
    "with tf.GradientTape() as tape_for_x, tf.GradientTape() as tape_for_y:\n",
    "    # Watching different variables with different tapes\n",
    "    tape_for_x.watch(x)\n",
    "    tape_for_y.watch(y)\n",
    "    \n",
    "    z = x * y\n",
    "\n",
    "dz_dx = tape_for_x.gradient(z, x)\n",
    "dz_dy = tape_for_y.gradient(z, y)\n",
    "print(f\"Gradient of z wrt x: {dz_dx}\")\n",
    "print(f\"Gradient of z wrt y: {dz_dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-picnic",
   "metadata": {
    "papermill": {
     "duration": 0.027877,
     "end_time": "2022-10-05T14:07:08.272952",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.245075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Higher order gradients\n",
    "\n",
    "Any computation done nside the `GradientTape` context gets recorded. If the computations involves gradient calculations, it gets recorded as well. This makes it easy to compute the `higher-order` gradients using the same API. Check this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "charged-clinton",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.339097Z",
     "iopub.status.busy": "2022-10-05T14:07:08.338376Z",
     "iopub.status.idle": "2022-10-05T14:07:08.349458Z",
     "shell.execute_reply": "2022-10-05T14:07:08.348838Z",
     "shell.execute_reply.started": "2022-10-05T13:56:19.101559Z"
    },
    "papermill": {
     "duration": 0.048222,
     "end_time": "2022-10-05T14:07:08.349618",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.301396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable x: 3.0\n",
      "\n",
      "Equation is y = x^3\n",
      "First Order Gradient wrt x (3 * x^2): 27.0\n",
      "Second Order Gradient wrt x (6^x): 18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0, name=\"x\")\n",
    "\n",
    "with tf.GradientTape() as tape1:\n",
    "    with tf.GradientTape() as tape0:\n",
    "        y = x * x * x\n",
    "    first_order_grad = tape0.gradient(y, x)\n",
    "second_order_grad = tape1.gradient(first_order_grad, x)\n",
    "\n",
    "print(f\"Variable x: {x.numpy()}\")\n",
    "print(\"\\nEquation is y = x^3\")\n",
    "print(f\"First Order Gradient wrt x (3 * x^2): {first_order_grad}\")\n",
    "print(f\"Second Order Gradient wrt x (6^x): {second_order_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-graham",
   "metadata": {
    "papermill": {
     "duration": 0.028399,
     "end_time": "2022-10-05T14:07:08.407134",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.378735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Gotchas\n",
    "\n",
    "Let's look at a few things that you **should** be aware of so that your code doesn't fail silently!\n",
    "\n",
    "We already looked at that gradients for `int` or `string` dtypes isn't defined. Here are a few other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "civilian-yugoslavia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.472675Z",
     "iopub.status.busy": "2022-10-05T14:07:08.471948Z",
     "iopub.status.idle": "2022-10-05T14:07:08.475206Z",
     "shell.execute_reply": "2022-10-05T14:07:08.475705Z",
     "shell.execute_reply.started": "2022-10-05T13:57:40.657683Z"
    },
    "papermill": {
     "duration": 0.03947,
     "end_time": "2022-10-05T14:07:08.475895",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.436425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "\n",
      "Gradient of y wrt x:  None\n"
     ]
    }
   ],
   "source": [
    "# What happens when you tries to take gradient wrt a Tensor?\n",
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * x\n",
    "    \n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(x)\n",
    "print(\"\\nGradient of y wrt x: \", dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "emotional-norfolk",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.541585Z",
     "iopub.status.busy": "2022-10-05T14:07:08.540718Z",
     "iopub.status.idle": "2022-10-05T14:07:08.544968Z",
     "shell.execute_reply": "2022-10-05T14:07:08.544178Z",
     "shell.execute_reply.started": "2022-10-05T13:57:47.126049Z"
    },
    "papermill": {
     "duration": 0.040109,
     "end_time": "2022-10-05T14:07:08.545184",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.505075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "\n",
      "Gradient of y wrt x:  tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Let's modify the above code a bit\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    \n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(x)\n",
    "print(\"\\nGradient of y wrt x: \", dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-permission",
   "metadata": {
    "papermill": {
     "duration": 0.028633,
     "end_time": "2022-10-05T14:07:08.603945",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.575312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Woah! What just happened? Don't look further down but pause for a minute and think for a while about what just happened and why such a behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-cheat",
   "metadata": {
    "papermill": {
     "duration": 0.028574,
     "end_time": "2022-10-05T14:07:08.661815",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.633241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### States and Gradients\n",
    "\n",
    "`GradientTape` can only read from the current state, not from the history that ead to it. State blocks gradient calculations from going farther back. Let's look at an example to make it more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "front-mining",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.729735Z",
     "iopub.status.busy": "2022-10-05T14:07:08.728850Z",
     "iopub.status.idle": "2022-10-05T14:07:08.740966Z",
     "shell.execute_reply": "2022-10-05T14:07:08.741619Z",
     "shell.execute_reply.started": "2022-10-05T14:02:18.836230Z"
    },
    "papermill": {
     "duration": 0.050212,
     "end_time": "2022-10-05T14:07:08.741813",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.691601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of z wrt y:  tf.Tensor(14.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(4.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Change the state of x by making x = x + y\n",
    "    \n",
    "    x.assign_add(y)\n",
    "    \n",
    "    # Let's do some computation e.g z = x * x \n",
    "    # This is equivalent to z = (x + y) * (x + y) because of above assign_add\n",
    "    # z= x^2 + y^2 +2x*y   dz/dx= 2x+2y=6+8=14\n",
    "    z = x * x\n",
    "    \n",
    "dx = tape.gradient(z, x)\n",
    "print(\"Gradients of z wrt y: \", dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "complex-composition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.804268Z",
     "iopub.status.busy": "2022-10-05T14:07:08.803478Z",
     "iopub.status.idle": "2022-10-05T14:07:08.814257Z",
     "shell.execute_reply": "2022-10-05T14:07:08.813611Z",
     "shell.execute_reply.started": "2022-10-05T14:04:54.570842Z"
    },
    "papermill": {
     "duration": 0.042922,
     "end_time": "2022-10-05T14:07:08.814428",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.771506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of z wrt x:  tf.Tensor(14.0, shape=(), dtype=float32)\n",
      "Gradients of z wrt y:  None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(4.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Change the state of x by making x = x + y\n",
    "    \n",
    "    x.assign_add(y)\n",
    "    \n",
    "    # Let's do some computation e.g z = x * x \n",
    "    # This is equivalent to z = (x + y) * (x + y) because of above assign_add\n",
    "    # z= x^2 + y^2 +2x*y   dz/dx= 2x+2y=6+8=14\n",
    "    z = x * x\n",
    "    \n",
    "dx,dy = tape.gradient(z, [x,y])\n",
    "print(\"Gradients of z wrt x: \", dx)\n",
    "print(\"Gradients of z wrt y: \", dy) # error no gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "overall-liquid",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:07:08.881202Z",
     "iopub.status.busy": "2022-10-05T14:07:08.880107Z",
     "iopub.status.idle": "2022-10-05T14:07:08.887851Z",
     "shell.execute_reply": "2022-10-05T14:07:08.888612Z",
     "shell.execute_reply.started": "2022-10-05T14:06:05.245991Z"
    },
    "papermill": {
     "duration": 0.044752,
     "end_time": "2022-10-05T14:07:08.888863",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.844111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of z wrt x:  tf.Tensor(14.0, shape=(), dtype=float32)\n",
      "Gradients of z wrt y:  tf.Tensor(14.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(4.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Change the state of x by making x = x + y\n",
    "    \n",
    "   \n",
    "    # Let's do some computation e.g z = x * x \n",
    "    # This is equivalent to z = (x + y) * (x + y) because of above assign_add\n",
    "    # z= x^2 + y^2 +2x*y   dz/dx= 2x+2y=6+8=14\n",
    "    z = (x + y) * (x + y)\n",
    "    \n",
    "dx,dy = tape.gradient(z, [x,y])\n",
    "print(\"Gradients of z wrt x: \", dx)\n",
    "print(\"Gradients of z wrt y: \", dy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-beginning",
   "metadata": {
    "papermill": {
     "duration": 0.030381,
     "end_time": "2022-10-05T14:07:08.950664",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.920283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That's it for part 3! I hope you liked the content and I am also hoping that it would have given you a much clear picture of Automatic Differentiation and Gradients calculation. We will be looking at other things in the next tutorial!<br>\n",
    "\n",
    "\n",
    "**References**:\n",
    "1. https://www.tensorflow.org/guide/autodiff\n",
    "2. https://keras.io/getting_started/intro_to_keras_for_researchers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-munich",
   "metadata": {
    "papermill": {
     "duration": 0.029538,
     "end_time": "2022-10-05T14:07:09.009982",
     "exception": false,
     "start_time": "2022-10-05T14:07:08.980444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.017858,
   "end_time": "2022-10-05T14:07:10.330473",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-05T14:06:51.312615",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
